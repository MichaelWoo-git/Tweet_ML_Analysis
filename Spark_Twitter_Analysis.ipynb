{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googledrivedownloader\n",
      "  Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB)\n",
      "Installing collected packages: googledrivedownloader\n",
      "Successfully installed googledrivedownloader-0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download File from link given in Canvas__\n",
    "\n",
    "This will be stored into your local...Do not add into git, file is too large to be pushed onto git master branch\n",
    "\n",
    "So, we download locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdd.download_file_from_google_drive(file_id='0B04GJPshIjmPRnZManQwWEdTZjg',\n",
    "                                    dest_path='/Users/swapnilbasu/Downloads/trainingandtestdata.zip',\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create spark session object (Data Processing)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('data_processing').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load in data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = spark.read.csv(\"/Users/swapnilbasu/Downloads/trainingandtestdata/training.1600000.processed.noemoticon.csv\",header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Renaming columns__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.toDF(\"target\",'id','date','query','user_name','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target', 'id', 'date', 'query', 'user_name', 'text']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Selecting the target value and text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = training_data.select('text','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|target|\n",
      "+--------------------+------+\n",
      "|@switchfoot http:...|     0|\n",
      "|is upset that he ...|     0|\n",
      "|@Kenichan I dived...|     0|\n",
      "|my whole body fee...|     0|\n",
      "|@nationwideclass ...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that its an even split between positive and negative tweets\n",
    "\n",
    "0: negative\n",
    "4: positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|target| count|\n",
      "+------+------+\n",
      "|     0|800000|\n",
      "|     4|800000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.groupBy(\"target\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Regular Expression Tokenizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", \n",
    "                                outputCol=\"words\", \n",
    "                                pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop Words Download from NLTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/swapnilbasu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop Words Remover__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "sp = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "extra_words = {\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"}\n",
    "for i in extra_words:\n",
    "    stop_words.add(i) \n",
    "stop_words = list(stop_words)\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", \n",
    "                                    outputCol=\"filtered\").setStopWords(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bag of words count__\n",
    "\n",
    "This is a type of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectors = CountVectorizer(inputCol=\"filtered\", \n",
    "                               outputCol=\"features\", \n",
    "                               vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__StringIndexer__\n",
    "\n",
    "This is where we create our new dataframe in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|                text|target|               words|            filtered|            features|label|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|@switchfoot http:...|     0|[switchfoot, http...|[switchfoot, twit...|(10000,[1,10,16,6...|  0.0|\n",
      "|is upset that he ...|     0|[is, upset, that,...|[upset, update, f...|(10000,[6,70,172,...|  0.0|\n",
      "|@Kenichan I dived...|     0|[kenichan, i, div...|[kenichan, dived,...|(10000,[4,213,251...|  0.0|\n",
      "|my whole body fee...|     0|[my, whole, body,...|[whole, body, fee...|(10000,[3,325,374...|  0.0|\n",
      "|@nationwideclass ...|     0|[nationwideclass,...|[nationwideclass,...|(10000,[20,486],[...|  0.0|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df)\n",
    "dataset = pipelineFit.transform(df)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Set seed for reproducibility__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 1119737\n",
      "Test Dataset Count: 480263\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+-----+----------+\n",
      "|                          text|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|sad sad sad sad sad sad sad...|[0.9987618635665929,0.00123...|  0.0|       0.0|\n",
      "|Slept bad, feel tired. Hate...|[0.9957913419884288,0.00420...|  0.0|       0.0|\n",
      "|super pissed that another t...|[0.9953102696313872,0.00468...|  0.0|       0.0|\n",
      "|I'm sooooo sick...sore thro...|[0.9933885939735362,0.00661...|  0.0|       0.0|\n",
      "|high temperature, runny nos...|[0.9929344107087643,0.00706...|  0.0|       0.0|\n",
      "|really bored and the pain k...|[0.990433624058968,0.009566...|  0.0|       0.0|\n",
      "|#trackle #trackle #trackle ...|[0.9893048053840303,0.01069...|  0.0|       0.0|\n",
      "|ugh I feel like crap. Heada...|[0.9890760738152531,0.01092...|  0.0|       0.0|\n",
      "|So yeah. I feel like shit. ...|[0.9880498997719068,0.01195...|  0.0|       0.0|\n",
      "|Im sick today yuck, i hate ...|[0.9877734010623013,0.01222...|  0.0|       0.0|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7706803149123818"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter is a book you'll never finish\n",
      "to everyone procrastinating, you've come to the right place\n",
      "cheers to all the Tweets that made it to IG this year\n",
      "your typo makes it original\n",
      "you're doing great, even if your Tweets aren't\n",
      "RT @TwitterBlue: It‚Äôs time to flex those Twitter fingers and take it to the next level üí™\n",
      "\n",
      "Twitter Blue is now available for subscription in‚Ä¶\n",
      "BIG NEWS lol jk still Twitter\n",
      "RT @TwitterSpaces: the time has arrived -- we‚Äôre now rolling out the ability for everyone on iOS and Android to host a Space\n",
      "\n",
      "if this is yo‚Ä¶\n",
      "‚ÄúI‚Äôm not on Twitter‚Äù üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©üö©\n",
      "the timeline is in retrograde\n",
      "RT @tmhsaysoo: We don‚Äôt beg for followers on twitter. You get em when you get em üò≠üò≠üò≠üò≠\n",
      "hello literally everyone\n",
      "apparently it's october\n",
      "oh you also love money? here‚Äôs how to send and receive Tips https://t.co/tCnzgrJEGE\n",
      "tested the Tips feature, turns out people love money\n",
      "\n",
      "rolling out on iOS with Android coming soon https://t.co/pkmLHzg6fu\n",
      "well, well, well\n",
      "ok but why should we follow you\n",
      "hello to everyone reading this while you should be doing something else\n",
      "virgos don‚Äôt need an edit button\n",
      "we were rooting for you, september\n",
      "Twitter\n",
      "60553128\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "ACCESS_TOKEN = \"1458842253779161088-QFeO6udaAdHR4VARxaDza1w4LUlooE\"\n",
    "ACCESS_TOKEN_SECRET = \"tC7IJDbl5T97Zvu3kE8sdGnmZWC2qxOrkdOv90YkdzIVO\"\n",
    "API_KEY = \"KLP5ct26qaVo0KjAgP8O4j4y5\"\n",
    "API_KEY_SECRET = \"AbxH3913WIPG0FHIwvVRomul92RWvuOdxRo2ecXR6H0Qgibo29\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_KEY_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)\n",
    "    \n",
    "user = api.get_user(screen_name='twitter')\n",
    "print(user.screen_name)\n",
    "print(user.followers_count)\n",
    "for friend in user.friends():\n",
    "   print(friend.screen_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21...\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SHOWE‚Ä¶\n",
      "quiero un spiderman ü•≤\n",
      "#SpiderMan #CheEve #NCT127_NEOCITY_THE_LINK\n",
      "#ENHYPEN_GayoDaechukje2021 #JISOO #GreysAnatomy #unprofessore #IndVsPak‚Ä¶ https://t.co/42r68SbOYv\n",
      "RT @Cena_The_Kepar: I some leak of spiderman no way home. #SpiderManNoWayHome \n",
      "\n",
      "RT+LIKE if you want it, i will send you a link in PM. \n",
      "\n",
      "Che‚Ä¶\n",
      "RT @Blacksheepash1: #SpiderManNoWayHome #spiderman\n",
      "When I finally see spiderman no way home and I can enter Twitter without worrying about‚Ä¶\n",
      "sampe nonton spiderman masih aja nontonnya sendiri https://t.co/ZgoWJiTnr3\n",
      "√áa me tue ma s≈ìur elle a rattrap√© les Spiderman qu‚Äôelle avait pas vue + Venom pour √™tre √† jour parce que je lui ai‚Ä¶ https://t.co/KQS3NbbKtT\n",
      "he visto Spiderman y he comido sushi no puedo ser m√°s feliz\n",
      "RT @0ievelynn: Eu entrando no Twitter depois de ter assistido o filme sem medo de tomar spoiler \n",
      "\n",
      "#SpiderManNoWayHome #SpiderMan \n",
      " https://‚Ä¶\n",
      "La actuaci√≥n de Tom Holland en Spiderman no way home fue √©pica un 10000/10\n",
      "@h4ruc0re Buenass, amanec√≠ bien y ahora a ver la nueva de spiderman ^^\n",
      "RT @martidenise_: Spiderman? De bokita https://t.co/ULOBzIbPIA\n",
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21...\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SHOWE‚Ä¶\n",
      "RT @chiarafer24: Happy #SpiderManNoWayHome Day! Be like Dr. Strange üòå #Spiderman #NoWayHome https://t.co/2Lx8sBJ1p9\n",
      "RT @slzyin: SPIDERMAN NWHM SKIN GAW:\n",
      "RT THIS AND LIKE QUOTED TWEET\n",
      "ENDS IN 10 Mins\n",
      "Do you think there will be a MCU Harry Osborn??\n",
      "#Marvel #DoctorStrange #SpiderMan\n",
      "RT @Pecas__: POD√âIS DEJAR DE HABLAR DE SPIDERMAN????\n",
      "RT @JaviBenedict: Una parte de m√≠ no est√° del todo segura si No Way Home es real o es un maravilloso e imposible delirio. Es todo lo que un‚Ä¶\n",
      "@GoobyThat lighning mc'queen gets bonded with venom and tobey maguire spiderman shows everyone in the audiance his small penis. have fun\n",
      "IM GOING TO SEE SPIDERMAN AT 9:40PM PFFFT\n",
      "@ordinaryykidd hloo Spiderman\n",
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21...\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SHOWE‚Ä¶\n",
      "RT @BonnieESL: La cara de Andrew despu√©s de salvar a MJ simplemente me rompe el coraz√≥n.\n",
      "#SpiderManNowWayHome #SpiderMan #SpiderVerse https‚Ä¶\n",
      "RT @MiaYim: I loved #SpiderMan  everyone should go watch it !\n",
      "Sledujte ‚ÄûSpiderman No Way Home Review | 100% Satisfaction Guaranteed‚Äú v slu≈æbe YouTube https://t.co/qYtLEd9aSw\n",
      "RT @Blacksheepash1: #SpiderManNoWayHome #spiderman\n",
      "When I finally see spiderman no way home and I can enter Twitter without worrying about‚Ä¶\n",
      "RT @Bhuwantastic: Climax scene from spiderman no way home... https://t.co/3bL4KAsLFh\n",
      "Me acabo de dar cuenta de que no he visto ninguna pel√≠cula de Spiderman y me invitaron a ver no way home hoy.üôÉ\n",
      "¬øQu√©‚Ä¶ https://t.co/FpcxOJHHz6\n",
      "RT @DimitryRO: Different stages of watching the new Spider-Man film #SpiderMan #SpiderManNowWayHome https://t.co/yQwOEbcHl3\n",
      "RT @MarvelDato: Con ustedes, amigos y amigas, el multiverso cinematogr√°fico de Marvel üòç\n",
      "\n",
      "#SpiderManNoWayHome \n",
      "#SpiderMan https://t.co/z2NYN‚Ä¶\n",
      "RT @chiarafer24: Happy #SpiderManNoWayHome Day! Be like Dr. Strange üòå #Spiderman #NoWayHome https://t.co/2Lx8sBJ1p9\n",
      "RT @LaGalera9: chicas fans de spiderman, las invito a mi cama para conocer el multisexo https://t.co/rmkG5DoPw7\n",
      "@17Kojou Die Spiderman Handschuhe sind for real das beste Mythic Item, das es bis jetzt gab\n",
      "RT @EzeChilo: no amigo como que este es el final de Spiderman https://t.co/liRJnIb74v\n",
      "H√°ganse un favor y miren la √∫ltima de Spiderman\n",
      "las 3 decadas loco. #SpiderMan https://t.co/twpqIDu00c\n",
      "RT @9zTeam: que rara est√° la nueva peli de Spiderman https://t.co/6T4RNrCwVc\n",
      "RT @astrobort: aca el enemigo no es ni scorsese ni spiderman es la pelicula horrible de avatar que sigue siendo la mas taquillera de la his‚Ä¶\n",
      "RT @LaGalera9: chicas fans de spiderman, las invito a mi cama para conocer el multisexo https://t.co/rmkG5DoPw7\n",
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21...\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SHOWE‚Ä¶\n",
      "Na en serio, la gente que est√°is con Spoilers de Spiderman No way home sois basura, que no han pasado ni 24 horas\n",
      "RT @astrobort: aca el enemigo no es ni scorsese ni spiderman es la pelicula horrible de avatar que sigue siendo la mas taquillera de la his‚Ä¶\n",
      "üòç v√≠deo e podcast gravados. ambos ir√£o sair domingo. o podcast √© o √°udio do v√≠deo do youtube. obrigado @_sevenlist‚Ä¶ https://t.co/CjtJLiWGXG\n",
      "RT @MScar8: Hoy voy con Spiderman a ver Spiderman ü•∫‚ù§Ô∏è\n",
      "can‚Äôt wait to watch the new spiderman today &lt;3\n",
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21...\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SHOWE‚Ä¶\n",
      "RT @slzyin: SPIDERMAN NWHM SKIN GAW:\n",
      "RT THIS AND LIKE QUOTED TWEET\n",
      "ENDS IN 10 Mins\n",
      "gapapa kok spiderman dan kawan2, gw tau rasanya gak keterima snm :‚Äô)\n",
      "RT @taran_adarsh: ‚≠êÔ∏è Thursday, non-holiday release\n",
      "‚≠êÔ∏è 50% capacity in Maharashtra \n",
      "‚≠êÔ∏è Pandemic era\n",
      "Yet, #SpiderMan takes a FANTABULOUS STAR‚Ä¶\n",
      "RT @gwnstacy_: anoche en el cine hab√≠an pibes de TRAJE esperando para ver spiderman que cosa maravillosa el var√≥n fan de peter parker eran‚Ä¶\n",
      "RT @ric_alarcon: Yo. Cuando apareci√≥ el spiderman de Andrew Garfield en #SpiderManNoWayHome https://t.co/DMdKGPhoZe\n",
      "Spiderman is just a furry\n",
      "#SpiderManNowWayHome spoilers without context...\n",
      "#SpiderMan #Spidey #SpiderManNoWayHomeleak https://t.co/ESgdkzXr71\n",
      "RT @ruesdaya: her spiderman.                        his mj. https://t.co/5Q7CQ0gHsJ\n",
      "RT @Polo__RMA: Tobey Maguire pour la nostalgie\n",
      "Andrew Garfield pour les √©motions\n",
      "Tom Holland pour l‚Äô√©volution\n",
      "@NotLukeRetweet holy crap lois it‚Äôs the spiderman guy\n",
      "RT @fabienciaga: I‚Äôve been in shock for the last 4 hours like spiderman nwh was fucking EXCELLENT marvel really ate that https://t.co/ZVTBn‚Ä¶\n",
      "RT @alywayss: te mereces a un novio que se crea spiderman y no fckboy\n",
      "RT @Polo__RMA: Tobey Maguire pour la nostalgie\n",
      "Andrew Garfield pour les √©motions\n",
      "Tom Holland pour l‚Äô√©volution\n",
      "RT @MBARI_News: Our spidey senses are tingling! üï∑Ô∏èüï∏Ô∏è‚Å† #SpiderMan\n",
      "‚Å†\n",
      "Like spiderman, giant sea spiders (aka pycnogonids), are not true spider‚Ä¶\n",
      "@C_Coleman10 @SpiderMan @Marvel @veve_official @EComi @Immutable @VeVeholic @johnnydunn777 @__TheGale__ Spoiler! üòµ‚Ä¶ https://t.co/4qSZKgTgTl\n",
      "RT @Kappah20: Voy a ver spiderman sin ningun spoiler en absoluto soy un puto dios\n",
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21...\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SHOWE‚Ä¶\n",
      "RT @rodopodprivate: #SpiderMan #SpiderManNoWayHome https://t.co/CUiFZrrwNM\n",
      "RT @lilidanielaaa: te mereces a un novio que se crea spiderman y no fuckboy\n",
      "RT @ruesdaya: her spiderman.                        his mj. https://t.co/5Q7CQ0gHsJ\n",
      "RT @nichole_paola: Necesito ver Spiderman de que pa ayerüò©\n",
      "RT @BRLeaks_ES: üï∑Ô∏è SORTEO 1 TRAJE DE SPIDERMAN (No Way Home) üï∑Ô∏è\n",
      "\n",
      "üìù Requisitos:\n",
      "‚Ä¢ Seguir a @BRLeaks_ES \n",
      "‚Ä¢ Seguir a @DeanCentenoTV \n",
      "‚Ä¢ RT a es‚Ä¶\n",
      "RT @rozainirahmat: i say alr, don't spoil spidermanü§¶üèª‚Äç‚ôÇÔ∏è\n",
      "RT @Pauglbrt: No voy a superar nunca Spiderman NWH https://t.co/AjSXAMfiSl\n",
      "BEST SPIDERMAN MOVIE HANDS DOWN\n",
      "When you want to talk about #SpiderMan but you have to wait until everyone has had time to see it. https://t.co/XhwvfZNtSV\n",
      "RT @jsanford: This is GREAT! Primo acting!\n",
      "#DoctorStrangeInTheMultiverseOfMadness\n",
      "quando eu levantei da cadeira as perna tava tudo bamba n√£o sei como desci as escadas\n",
      "RT @CLINT419: never goin cinema in shoreditch again the person next to me ordered a fucking latte to watch spiderman wheres the tango fucki‚Ä¶\n",
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21......\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SH‚Ä¶\n",
      "RT @chicagobulls: ‚ÄúWith great power comes great responsibility.‚Äù #SpiderMan https://t.co/QHzCVjuA29\n",
      "tw: NO WAY HOME SPOILERS // lit llor√© con esta escena\n",
      "This fucking scene bring back my childhood man, \n",
      "THIS PARTICULAR SCENE!!\n",
      "RT @Cesar_ST_: Cuando ya fuiste a ver #SpiderMan y ya eres inmune a Spoilers ‚úåüï∑üï∏ #SpiderManNoWayHome https://t.co/V6kcyapuZb\n",
      "RT @Blacksheepash1: #SpiderManNoWayHome #spiderman\n",
      "When I finally see spiderman no way home and I can enter Twitter without worrying about‚Ä¶\n",
      "RT @Rever1s: Spiderman (No Way Home) Collab Skin Giveaway:\n",
      "\n",
      "- Retweet\n",
      "- Follow Me &amp; @ImaadGives\n",
      "\n",
      "Ends in 4 Hours, Good Luck üçÄ https://t.co/‚Ä¶\n",
      "RT @delaocadiego: Yo cont√°ndole a mi mam√° que tal me pareci√≥ #SpiderManNoWayHome \n",
      "\n",
      "#SpiderMan https://t.co/a98R5aoiWW\n",
      "Im gonna watch spiderman Sunday \n",
      "https://t.co/buRTjn8hyS\n",
      "Not a spoiler, but there‚Äôs a moment where a character does something they couldn‚Äôt do before and the raw emotion fr‚Ä¶ https://t.co/VwbLIotkEo\n",
      "Props to #SpiderMan #NoWayHome  for bringing in Tobey Maguire  and Jon Bernthal just to do this scene #RIPSpidy https://t.co/k3RmgsInmW\n",
      "RT @unicorniocanica: ya vi spiderman no way home y estoy temblando vomitando y sacudi√©ndome porque alfred molina nunca dijo ‚Äúme extra√±a que‚Ä¶\n",
      "RT @Nanxss: Pr√©cision ! Ce sera des Cartes de 2800 V-Bucks que je donnerait aux gagnants ! üî•\n",
      "RT @JusNormalPerson: Spoiler\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "Andrew tobey entry \n",
      "In INDIA üáÆüá≥\n",
      "\n",
      "#SpiderManNoWayHome \n",
      "#Spiderman https://t.co/lfIrdQT5By\n",
      "#SpiderManNowWayHome #SpiderMan  #BackToTheFuture @kyloftus \n",
      "\n",
      "Review: \"Spider-Man: No Way Home\" strains under plot,‚Ä¶ https://t.co/SKLxlRkLHp\n",
      "üñï\n",
      "RT @tyunhq: NEXT PERSON WHO TALKS BADLY ABOUT TUBATU IS RECEIVING A PLOT SUMMARY OF SPIDERMAN NO WAY HOME IN THEIR DMS.\n",
      "@Tg3web #SpiderMan \n",
      "... https://t.co/ULklBT5gHY\n",
      "RT @chiarafer24: Happy #SpiderManNoWayHome Day! Be like Dr. Strange üòå #Spiderman #NoWayHome https://t.co/2Lx8sBJ1p9\n",
      "puede todo el mundo ver spiderman nwh porque quiero hablar de la peli pero no quiero hacer spoilers üòìüëéüèª\n",
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21......\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SH‚Ä¶\n",
      "RT @rodopodprivate: #SpiderMan #SpiderManNoWayHome https://t.co/CUiFZrrwNM\n",
      "ma√±ana voy a ver spidermanüòΩüòΩ\n",
      "RT @hariszam: Kena tahan polis dekat roadblock. \n",
      "\n",
      "\"Dari mana encik?\"\n",
      "\"Balik dari wayang tengok spiderman.\"\n",
      "\"Okay\"\n",
      "\"Encik nak spoiler tak?\"‚Ä¶\n",
      "RT @SpiderMan: Willem Dafoe (Green Goblin), Alfred Molina (Doc Ock), and @IAmJamieFoxx (Electro) discuss where their characters pick up in‚Ä¶\n",
      "RT @delaocadiego: Yo cont√°ndole a mi mam√° que tal me pareci√≥ #SpiderManNoWayHome \n",
      "\n",
      "#SpiderMan https://t.co/a98R5aoiWW\n",
      "@INOXSupport Will there be Spiderman: No Way Home at IMAX 3D from Wednesday, 22/12/2021 in Bengaluru? since The Mat‚Ä¶ https://t.co/5kNyUbUDmb\n",
      "RT @Electroalces: Esto es totalmente te real ü§£ü§£ü§£\n",
      "#Marvel #SpiderManNoWayHome #Spiderman #NoWayHome #UCM https://t.co/EA4GUt7LZ3\n",
      "RT @momo58_: Le seul et unique vrai spiderman https://t.co/PtpXlBME06\n",
      "RT @ruesdaya: her spiderman.                        his mj. https://t.co/5Q7CQ0gHsJ\n",
      "RT @bigscreenleaks: Growing up reading, watching and learning everything related to #SpiderMan, I'm at a loss for words right now. #SpiderM‚Ä¶\n",
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21......\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SH‚Ä¶\n",
      "RT @justinloIII: am i supposed to watch spiderman by myself or wtf???\n",
      "RT @Saltvador: Yo: ay que cringe que se vayan disfrazados de Spiderman üòµ‚Äçüí´\n",
      "Yo en el Born This Way Ball: https://t.co/crxqz0K31o\n",
      "RT @ruesdaya: her spiderman.                        his mj. https://t.co/5Q7CQ0gHsJ\n",
      "Je le deteste cordialement\n",
      "Nonton spiderman yuk,\n",
      "Tp ntr kalo aku kedinginan di bioskop ‚Äúpegangin tgn aku ya?‚Äùüòã\n",
      "RT @Kenzo132: Bah l‚Äôoriginal quoi.. pas besoin d‚Äôartefacts pour balancer ses toiles etc. Celui qui se rapproche le plus de celui des Comics\n",
      "I'm probably gonna watch the new Spiderman movie close to the end of it's airing when the hype dies down so please‚Ä¶ https://t.co/DR4BVXzC4c\n",
      "RT @sofiajohnsonm: El hecho de que salvo a MJ pensando en Gwen #spiderman #SpiderVerse https://t.co/vFVyYBZ83o\n",
      "@disl3xia Aparece el Spiderman de Adam West\n",
      "Volvi√≥ El Amante.\n",
      "RT @Warlockdnd: Leaked footage of the next Spider-Man movie #SpiderMan https://t.co/Yp1quzqdxt\n",
      "RT @AslionB: @zatheo_ @thoomas_off En r√©ponse √† @zatheo_ et @thoomas_off  je suis fan de spiderman depuis longtemps et j'aimerais vrm gagne‚Ä¶\n",
      "RT @WohltatTV: New Fortnite Update! Are Spiderman Challenges coming?\n",
      "Que bueno que no soy fan√°tico de Spiderman xD andar√≠a sufriendo ahorita üòÇü§ß\n",
      "RT @CalTh0mas2100: i will just say #SpiderManNoWayHome turned Tom Holland from ‚Äòthe mcu spider-man‚Äô into THE #SpiderMan REAL QUICK https://‚Ä¶\n",
      "RT @axrual: SPOILER SPIDERMAN NO WAY HOME https://t.co/EvcUPQkeDH\n",
      "RT @antroshouse: #AntrosShower my SEX home\n",
      "putos safados e leiteiros est√£o convocados! \n",
      "DIA 17/12/21......\n",
      "√°s 15:30H at√© 22h\n",
      "LOCAL SAUNA SH‚Ä¶\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-84071bd99b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Filter realtime Tweets by keyword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Spiderman\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, follow, track, locations, filter_level, languages, stall_warnings, threaded)\u001b[0m\n\u001b[1;32m    268\u001b[0m                                           body=body)\n\u001b[1;32m    269\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstall_warnings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, method, endpoint, params, headers, body)\u001b[0m\n\u001b[1;32m    127\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                             for line in resp.iter_lines(\n\u001b[0m\u001b[1;32m    130\u001b[0m                                 \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                             ):\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36miter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0mpending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_unicode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_unicode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpending\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \"\"\"\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Subclass Stream to print IDs of Tweets received\n",
    "class IDPrinter(tweepy.Stream):\n",
    "\n",
    "    def on_status(self, status):\n",
    "        print(status.text)\n",
    "\n",
    "# Initialize instance of the subclass\n",
    "printer = IDPrinter(\n",
    "  API_KEY, API_KEY_SECRET,\n",
    "  ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    ")\n",
    "\n",
    "# Filter realtime Tweets by keyword\n",
    "printer.filter(track=[\"Spiderman\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
