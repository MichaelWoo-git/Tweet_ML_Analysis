{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download File from link given in Canvas__\n",
    "\n",
    "This will be stored into your local...Do not add into git, file is too large to be pushed onto git master branch\n",
    "\n",
    "So, we download locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdd.download_file_from_google_drive(file_id='0B04GJPshIjmPRnZManQwWEdTZjg',\n",
    "                                    dest_path='/Users/mwoo/Downloads/trainingandtestdata.zip',\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdd.download_file_from_google_drive(file_id='0B04GJPshIjmPRnZManQwWEdTZjg',\n",
    "#                                     dest_path='/Users/swapnilbasu/Downloads/trainingandtestdata.zip',\n",
    "#                                     unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create spark session object (Data Processing)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('classification_tweet').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load in data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = spark.read.csv(\"/Users/mwoo/Downloads/training.1600000.processed.noemoticon.csv\",header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Renaming columns__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.toDF(\"target\",'id','date','query','user_name','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target', 'id', 'date', 'query', 'user_name', 'text']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exploratory__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, target: string, id: string, date: string, query: string, user_name: string, text: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Selecting the target value and text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = training_data.select('text','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|target|\n",
      "+--------------------+------+\n",
      "|@switchfoot http:...|     0|\n",
      "|is upset that he ...|     0|\n",
      "|@Kenichan I dived...|     0|\n",
      "|my whole body fee...|     0|\n",
      "|@nationwideclass ...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that its an even split between positive and negative tweets\n",
    "\n",
    "0: negative\n",
    "4: positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|target| count|\n",
      "+------+------+\n",
      "|     0|800000|\n",
      "|     4|800000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.groupBy(\"target\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Regular Expression Tokenizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop Words Download from NLTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mwoo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop Words Remover__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "sp = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "extra_words = {\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"}\n",
    "for i in extra_words:\n",
    "    stop_words.add(i) \n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bag of words count__\n",
    "\n",
    "This is a type of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__StringIndexer__\n",
    "\n",
    "This is where we create our new dataframe in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|                text|target|               words|            filtered|            features|label|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|@switchfoot http:...|     0|[switchfoot, http...|[switchfoot, twit...|(10000,[1,10,16,6...|  0.0|\n",
      "|is upset that he ...|     0|[is, upset, that,...|[upset, update, f...|(10000,[6,70,172,...|  0.0|\n",
      "|@Kenichan I dived...|     0|[kenichan, i, div...|[kenichan, dived,...|(10000,[4,213,251...|  0.0|\n",
      "|my whole body fee...|     0|[my, whole, body,...|[whole, body, fee...|(10000,[3,325,374...|  0.0|\n",
      "|@nationwideclass ...|     0|[nationwideclass,...|[nationwideclass,...|(10000,[20,486],[...|  0.0|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "# Fit the pipeline to training documents\n",
    "pipelineFit = pipeline.fit(df)\n",
    "dataset = pipelineFit.transform(df)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select('text','features','label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Set seed for reproducibility__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "# print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "# print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(10000,[1,10,16,6...|  0.0|\n",
      "|(10000,[6,70,172,...|  0.0|\n",
      "|(10000,[4,213,251...|  0.0|\n",
      "|(10000,[3,325,374...|  0.0|\n",
      "|(10000,[20,486],[...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df = dataset.select('features','label')\n",
    "model_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingData = dataset\n",
    "# print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "# # print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0).fit(model_df)\n",
    "lr.save(\"Users/mwoo/lr_model_1\")\n",
    "# lrModel = lr.fit(trainingData)\n",
    "#predictions = lrModel.transform(testData)\n",
    "#predictions.filter(predictions['prediction'] == 0) \\\n",
    "#     .select(\"text\",\"probability\",\"label\",\"prediction\") \\\n",
    "#     .orderBy(\"probability\", ascending=False) \\\n",
    "#     .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "# evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Format Test Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "ACCESS_TOKEN = \"1458842253779161088-QFeO6udaAdHR4VARxaDza1w4LUlooE\"\n",
    "ACCESS_TOKEN_SECRET = \"tC7IJDbl5T97Zvu3kE8sdGnmZWC2qxOrkdOv90YkdzIVO\"\n",
    "API_KEY = \"KLP5ct26qaVo0KjAgP8O4j4y5\"\n",
    "API_KEY_SECRET = \"AbxH3913WIPG0FHIwvVRomul92RWvuOdxRo2ecXR6H0Qgibo29\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_KEY_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stream connection closed by Twitter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Madelei05642209: Nice weekend \n",
      "\n",
      "More links in bio https://t.co/EicnkmON8z\n",
      "@itstylersays @valueandtime lemons &gt; rugs\n",
      "RT @bvanhoovan: SCRAPPY is skinny, has lost over 20 pounds and you can see her spine... https://t.co/1ebmu0sh7e\n",
      "@hndrxhours Preciate it\n",
      "RT @EricaSimonee: u gon make me turn up on youuuuu\n",
      "RT @THOUXANBANKY: who likes big tiddies? https://t.co/eoqX0jQOll\n",
      "@TheRealForno Bailey Zappe is a borderline submarine for how under the radar he's been flying\n",
      "@WORLDMUSICAWARD @VGlobalUnion @BTS_twt You too whipped for him huh?!? Omg this man is driving the world insane. ðŸ˜­\n",
      "@DrewLawDesign It i think itâ€™s same accounts i report.\n",
      "RT @luminecity: paimon jokes about being the goddess of protection but is she REALLY joking? the traveler has gone through multiple life orâ€¦\n"
     ]
    }
   ],
   "source": [
    "tweet_list = list()\n",
    "# Subclass Stream to print IDs of Tweets received\n",
    "class IDPrinter(tweepy.Stream):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        tweet_list.append(status.text)\n",
    "        #print(tweet_list)\n",
    "        print(status.text)\n",
    "        if len(tweet_list) == 10:\n",
    "            Stream.disconnect(self)\n",
    "# Initialize instance of the subclass\n",
    "printer = IDPrinter(\n",
    "  API_KEY, API_KEY_SECRET,\n",
    "  ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    ")\n",
    "\n",
    "# Filter realtime Tweets by keyword\n",
    "# printer.filter(track=[\"Spiderman\"])\n",
    "printer.sample(languages=['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame(np.array(tweet_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = spark.read.csv('test.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|RT @Madelei056422...|\n",
      "|More links in bio...|\n",
      "|@itstylersays @va...|\n",
      "|RT @bvanhoovan: S...|\n",
      "|@hndrxhours Preci...|\n",
      "|RT @EricaSimonee:...|\n",
      "|RT @THOUXANBANKY:...|\n",
      "|@TheRealForno Bai...|\n",
      "|@WORLDMUSICAWARD ...|\n",
      "|@DrewLawDesign It...|\n",
      "|RT @luminecity: p...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|               words|            filtered|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|RT @Madelei056422...|[rt, madelei05642...|[madelei05642209,...|(10000,[60,79],[1...|\n",
      "|More links in bio...|[more, links, in,...|[links, bio, co, ...|(10000,[1122,1739...|\n",
      "|@itstylersays @va...|[itstylersays, va...|[itstylersays, va...| (10000,[270],[1.0])|\n",
      "|RT @bvanhoovan: S...|[rt, bvanhoovan, ...|[bvanhoovan, scra...|(10000,[20,154,54...|\n",
      "|@hndrxhours Preci...|[hndrxhours, prec...|[hndrxhours, prec...|       (10000,[],[])|\n",
      "|RT @EricaSimonee:...|[rt, ericasimonee...|[ericasimonee, u,...|(10000,[14,54,594...|\n",
      "|RT @THOUXANBANKY:...|[rt, thouxanbanky...|[thouxanbanky, li...|(10000,[157,1122,...|\n",
      "|@TheRealForno Bai...|[therealforno, ba...|[therealforno, ba...|(10000,[1461,6071...|\n",
      "|@WORLDMUSICAWARD ...|[worldmusicaward,...|[worldmusicaward,...|(10000,[118,140,2...|\n",
      "|@DrewLawDesign It...|[drewlawdesign, i...|[drewlawdesign, t...|(10000,[27,1493,2...|\n",
      "|RT @luminecity: p...|[rt, luminecity, ...|[luminecity, paim...|(10000,[19,99,232...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_1 = pipelineFit.transform(df_2)\n",
    "dataset_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = dataset_1.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o847.load.\n: java.lang.NoSuchMethodException: org.apache.spark.ml.classification.LogisticRegressionModel.<init>(java.lang.String)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3427)\n\tat java.base/java.lang.Class.getConstructor(Class.java:2165)\n\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:468)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-75906ce9e241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Users/mwoo/lr_model_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a string, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_from_java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o847.load.\n: java.lang.NoSuchMethodException: org.apache.spark.ml.classification.LogisticRegressionModel.<init>(java.lang.String)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3427)\n\tat java.base/java.lang.Class.getConstructor(Class.java:2165)\n\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:468)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n"
     ]
    }
   ],
   "source": [
    "#lr = LogisticRegression.load(\"Users/mwoo/lr_model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = lr.transform(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|(10000,[60,79],[1...|[-0.4281486250091...|[0.39456850936686...|       1.0|\n",
      "|(10000,[1122,1739...|[-0.1648060284513...|[0.45889149655844...|       1.0|\n",
      "| (10000,[270],[1.0])|[-0.0885826444741...|[0.47786880873173...|       1.0|\n",
      "|(10000,[20,154,54...|[0.21449568197955...|[0.55341926613738...|       0.0|\n",
      "|       (10000,[],[])|[-0.0928704763649...|[0.47679905404779...|       1.0|\n",
      "|(10000,[14,54,594...|[0.12539457939191...|[0.53130763268025...|       0.0|\n",
      "|(10000,[157,1122,...|[-0.3916937325633...|[0.40330963564820...|       1.0|\n",
      "|(10000,[1461,6071...|[-0.0827977819452...|[0.47931237178605...|       1.0|\n",
      "|(10000,[118,140,2...|[-0.1839235009762...|[0.45414830730536...|       1.0|\n",
      "|(10000,[27,1493,2...|[0.18043210896929...|[0.54498604721907...|       0.0|\n",
      "|(10000,[19,99,232...|[0.13168617609362...|[0.53287405132235...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
