{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download File from link given in Canvas__\n",
    "\n",
    "This will be stored into your local...Do not add into git, file is too large to be pushed onto git master branch\n",
    "\n",
    "So, we download locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdd.download_file_from_google_drive(file_id='0B04GJPshIjmPRnZManQwWEdTZjg',\n",
    "                                    dest_path='/Users/mwoo/Downloads/trainingandtestdata.zip',\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdd.download_file_from_google_drive(file_id='0B04GJPshIjmPRnZManQwWEdTZjg',\n",
    "#                                     dest_path='/Users/swapnilbasu/Downloads/trainingandtestdata.zip',\n",
    "#                                     unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create spark session object (Data Processing)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('classification_tweet').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load in data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = spark.read.csv(\"/Users/mwoo/Downloads/training.1600000.processed.noemoticon.csv\",header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Renaming columns__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.toDF(\"target\",'id','date','query','user_name','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target', 'id', 'date', 'query', 'user_name', 'text']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exploratory__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, target: string, id: string, date: string, query: string, user_name: string, text: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Selecting the target value and text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = training_data.select('text','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|target|\n",
      "+--------------------+------+\n",
      "|@switchfoot http:...|     0|\n",
      "|is upset that he ...|     0|\n",
      "|@Kenichan I dived...|     0|\n",
      "|my whole body fee...|     0|\n",
      "|@nationwideclass ...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that its an even split between positive and negative tweets\n",
    "\n",
    "0: negative\n",
    "4: positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|target| count|\n",
      "+------+------+\n",
      "|     0|800000|\n",
      "|     4|800000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"target\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Pipeline__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Regular Expression Tokenizer__\n",
    "\n",
    "Seperates the texts into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop Words Download from NLTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mwoo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop Words Remover__\n",
    "\n",
    "Remove unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "extra_words = {\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"}\n",
    "for i in extra_words:\n",
    "    stop_words.add(i) \n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bag of words count__\n",
    "\n",
    "This is a type of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__StringIndexer__\n",
    "\n",
    "Indexing target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|                text|target|               words|            filtered|            features|label|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|@switchfoot http:...|     0|[switchfoot, http...|[switchfoot, twit...|(10000,[1,10,16,6...|  0.0|\n",
      "|is upset that he ...|     0|[is, upset, that,...|[upset, update, f...|(10000,[6,70,172,...|  0.0|\n",
      "|@Kenichan I dived...|     0|[kenichan, i, div...|[kenichan, dived,...|(10000,[4,213,251...|  0.0|\n",
      "|my whole body fee...|     0|[my, whole, body,...|[whole, body, fee...|(10000,[3,325,374...|  0.0|\n",
      "|@nationwideclass ...|     0|[nationwideclass,...|[nationwideclass,...|(10000,[20,486],[...|  0.0|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(df)\n",
    "dataset = pipelineFit.transform(df)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Selecting data from the previous dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select('text','features','label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Set seed for reproducibility__\n",
    "\n",
    "This set is use for testing purposes 70/30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 1120280\n",
      "Test Dataset Count: 479720\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be used to fully train our classifcation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Training Dataset Count: 1600000\n"
     ]
    }
   ],
   "source": [
    "model_df = dataset.select('features','label')\n",
    "print(\"Full Training Dataset Count: \" + str(model_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Testing our model through the split data above__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+-----+----------+\n",
      "|                          text|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|@KoolioHoolio see i didnt e...|[0.9983510746371353,0.00164...|  1.0|       0.0|\n",
      "|you suck you suck you suck ...|[0.9956434658325106,0.00435...|  0.0|       0.0|\n",
      "|super pissed that another t...|[0.9952914880302268,0.00470...|  0.0|       0.0|\n",
      "|Things I'm feeling now: ang...|[0.9942653689730412,0.00573...|  0.0|       0.0|\n",
      "|so sad, me equal sad, no so...|[0.9926124136068198,0.00738...|  0.0|       0.0|\n",
      "|is feeling sad and stressed...|[0.9921178408523211,0.00788...|  0.0|       0.0|\n",
      "|today i kinda feel sick of ...|[0.9918232711532882,0.00817...|  0.0|       0.0|\n",
      "|Been sick with sore throat ...|[0.9901000573760685,0.00989...|  0.0|       0.0|\n",
      "|Throat is killing me, runny...|[0.9884492105393018,0.01155...|  0.0|       0.0|\n",
      "|Ugh my nose is stuffy, my t...|[0.98836386343389,0.0116361...|  0.0|       0.0|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|                text|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|       i really2 ...|(10000,[3,5741],[...|  0.0|[0.20571850757733...|[0.55124901560658...|       0.0|\n",
      "|     jb isnt show...|(10000,[856,1150,...|  0.0|[0.46727734120383...|[0.61473913745914...|       0.0|\n",
      "|     ok thats it ...|(10000,[98,200,31...|  0.0|[-0.3318046393922...|[0.41780159149143...|       1.0|\n",
      "|    Not feeling i...|(10000,[6,54,104,...|  0.0|[1.83967462148337...|[0.86291022093666...|       0.0|\n",
      "|    on the comput...|(10000,[11,295,36...|  0.0|[0.16413012819804...|[0.54094066593116...|       0.0|\n",
      "|   BoRinG   ): wh...|(10000,[92,175,29...|  0.0|[0.65692733517103...|[0.65856981953030...|       0.0|\n",
      "|   Boston Globe c...|(10000,[66,72,222...|  0.0|[0.43364162446366...|[0.60674291862623...|       0.0|\n",
      "|   My phone can u...|(10000,[4,39,124,...|  0.0|[0.35291827666599...|[0.58732507406826...|       0.0|\n",
      "|   hoping to see ...|(10000,[20,171,17...|  0.0|[-0.1493100173349...|[0.46274168808460...|       1.0|\n",
      "|   i'm so cold th...|(10000,[35,223,70...|  0.0|[0.39765886364928...|[0.59812504721712...|       0.0|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0).fit(trainingData)\n",
    "predictions = lr.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "predictions = lr.transform(testData)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8471903651669457\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bce = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(bce.evaluate(predictions, {bce.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection = predictions.select(\"label\",'prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.75      0.77    239942\n",
      "         1.0       0.76      0.80      0.78    239778\n",
      "\n",
      "    accuracy                           0.77    479720\n",
      "   macro avg       0.77      0.77      0.77    479720\n",
      "weighted avg       0.77      0.77      0.77    479720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "true = np.array(df_selection['label'])\n",
    "pred = np.array(df_selection['prediction'])\n",
    "print(classification_report(true,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can say that this model is able to distinguish whether a tweet is positive or negative is fair__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Retrain our model using the fully dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0).fit(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Twitter Authentication__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "ACCESS_TOKEN = \"1458842253779161088-QFeO6udaAdHR4VARxaDza1w4LUlooE\"\n",
    "ACCESS_TOKEN_SECRET = \"tC7IJDbl5T97Zvu3kE8sdGnmZWC2qxOrkdOv90YkdzIVO\"\n",
    "API_KEY = \"KLP5ct26qaVo0KjAgP8O4j4y5\"\n",
    "API_KEY_SECRET = \"AbxH3913WIPG0FHIwvVRomul92RWvuOdxRo2ecXR6H0Qgibo29\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_KEY_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Twitter Streaming Tweets__\n",
    "\n",
    "We can set the limit of tweet samples to 10 and where the streaming only streams tweets that are in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stream connection closed by Twitter\n"
     ]
    }
   ],
   "source": [
    "tweet_list = list()\n",
    "# Subclass Stream to print IDs of Tweets received\n",
    "class IDPrinter(tweepy.Stream):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        tweet_list.append(status.text)\n",
    "        #print(tweet_list)\n",
    "        #print(status.text)\n",
    "        if len(tweet_list) == 100:\n",
    "            Stream.disconnect(self)\n",
    "# Initialize instance of the subclass\n",
    "printer = IDPrinter(\n",
    "  API_KEY, API_KEY_SECRET,\n",
    "  ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    ")\n",
    "\n",
    "printer.sample(languages=['en'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create new dataframe from tweet stream__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|RT @JesuitTigers_...|\n",
      "|Disabled Vehicle:...|\n",
      "|RT @NCTsmtown: SM...|\n",
      "|RT @keyon: BEST S...|\n",
      "|    @oha_yanii evett|\n",
      "|RT @emilyeveryep:...|\n",
      "|RT @grimnorth_0: ...|\n",
      "|@EssexPR That sai...|\n",
      "|RT @jenoverse423:...|\n",
      "|RT @aishlut: snea...|\n",
      "|RT @chitaglorya__...|\n",
      "|         every look.|\n",
      "|cross solar cooke...|\n",
      "|RT @RRRMovie: Fin...|\n",
      "|Mis outfits han e...|\n",
      "|RT @cryptogems555...|\n",
      "|RT @AutomizedCock...|\n",
      "|RT @shootsgard: P...|\n",
      "|@TTLadyLuscious L...|\n",
      "|RT @Pratikfc7: Ne...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = pd.DataFrame(np.array(tweet_list))\n",
    "df_2.columns = ['text']\n",
    "df_2 = spark.createDataFrame(df_2)\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Transforms the data received from stream__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|               words|            filtered|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|RT @JesuitTigers_...|[rt, jesuittigers...|[jesuittigers_fb,...|(10000,[511,651,1...|\n",
      "|Disabled Vehicle:...|[disabled, vehicl...|[disabled, vehicl...|(10000,[23,452,51...|\n",
      "|RT @NCTsmtown: SM...|[rt, nctsmtown, s...|[nctsmtown, smtow...|(10000,[868,3189]...|\n",
      "|RT @keyon: BEST S...|[rt, keyon, best,...|[keyon, best, spi...|(10000,[94,118,13...|\n",
      "|    @oha_yanii evett|  [oha_yanii, evett]|  [oha_yanii, evett]|       (10000,[],[])|\n",
      "|RT @emilyeveryep:...|[rt, emilyeveryep...|[emilyeveryep, 5x...|(10000,[3,10,42,3...|\n",
      "|RT @grimnorth_0: ...|[rt, grimnorth_0,...|[grimnorth_0, bma...|(10000,[23,32,103...|\n",
      "|@EssexPR That sai...|[essexpr, that, s...|[essexpr, said, w...|(10000,[6,24,127,...|\n",
      "|RT @jenoverse423:...|[rt, jenoverse423...|[jenoverse423, he...|(10000,[25,123,34...|\n",
      "|RT @aishlut: snea...|[rt, aishlut, sne...|[aishlut, sneaky,...|(10000,[1122,6844...|\n",
      "|RT @chitaglorya__...|[rt, chitaglorya_...|[chitaglorya__, 3...|(10000,[263,318,7...|\n",
      "|         every look.|       [every, look]|       [every, look]|(10000,[126,299],...|\n",
      "|cross solar cooke...|[cross, solar, co...|[cross, solar, co...|(10000,[191,2039,...|\n",
      "|RT @RRRMovie: Fin...|[rt, rrrmovie, fi...|[rrrmovie, final,...|(10000,[1,554,124...|\n",
      "|Mis outfits han e...|[mis, outfits, ha...|[mis, outfits, ha...|(10000,[335,680,1...|\n",
      "|RT @cryptogems555...|[rt, cryptogems55...|[cryptogems555, c...|(10000,[15,22,64,...|\n",
      "|RT @AutomizedCock...|[rt, automizedcoc...|[automizedcock, l...|(10000,[3,17,39,6...|\n",
      "|RT @shootsgard: P...|[rt, shootsgard, ...|[shootsgard, plea...|(10000,[92,129,14...|\n",
      "|@TTLadyLuscious L...|[ttladyluscious, ...|[ttladyluscious, ...|(10000,[968,1719,...|\n",
      "|RT @Pratikfc7: Ne...|[rt, pratikfc7, n...|[pratikfc7, never...|(10000,[76,83,93,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_1 = pipelineFit.transform(df_2)\n",
    "dataset_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Selecting text and Features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_1.select('text','features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Having the pre-trained model predict on tweets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = lr.transform(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|RT @JesuitTigers_...|(10000,[511,651,1...|[-0.5318821511759...|[0.37007801323871...|       1.0|\n",
      "|Disabled Vehicle:...|(10000,[23,452,51...|[0.94578290396596...|[0.72026629565034...|       0.0|\n",
      "|RT @NCTsmtown: SM...|(10000,[868,3189]...|[-0.1436763568720...|[0.46414257299695...|       1.0|\n",
      "|RT @keyon: BEST S...|(10000,[94,118,13...|[0.05274802373889...|[0.51318394920864...|       0.0|\n",
      "|    @oha_yanii evett|       (10000,[],[])|[-0.0928899453666...|[0.47679419727941...|       1.0|\n",
      "|RT @emilyeveryep:...|(10000,[3,10,42,3...|[0.40655781964782...|[0.60026222209011...|       0.0|\n",
      "|RT @grimnorth_0: ...|(10000,[23,32,103...|[1.02569250257777...|[0.73607994580437...|       0.0|\n",
      "|@EssexPR That sai...|(10000,[6,24,127,...|[0.59996286217951...|[0.64564780963178...|       0.0|\n",
      "|RT @jenoverse423:...|(10000,[25,123,34...|[-1.2133129201857...|[0.22911539287758...|       1.0|\n",
      "|RT @aishlut: snea...|(10000,[1122,6844...|[-0.2506113912790...|[0.43767302060754...|       1.0|\n",
      "|RT @chitaglorya__...|(10000,[263,318,7...|[-0.1435508020280...|[0.46417380041572...|       1.0|\n",
      "|         every look.|(10000,[126,299],...|[-0.2265889462370...|[0.44359389329476...|       1.0|\n",
      "|cross solar cooke...|(10000,[191,2039,...|[-0.4869611165590...|[0.38060971177058...|       1.0|\n",
      "|RT @RRRMovie: Fin...|(10000,[1,554,124...|[-0.4318367434652...|[0.39368781958615...|       1.0|\n",
      "|Mis outfits han e...|(10000,[335,680,1...|[0.03483044916159...|[0.50870673208646...|       0.0|\n",
      "|RT @cryptogems555...|(10000,[15,22,64,...|[-0.3794205052467...|[0.40626667175369...|       1.0|\n",
      "|RT @AutomizedCock...|(10000,[3,17,39,6...|[0.23827294014903...|[0.55928799872501...|       0.0|\n",
      "|RT @shootsgard: P...|(10000,[92,129,14...|[1.47314794077036...|[0.81353538606218...|       0.0|\n",
      "|@TTLadyLuscious L...|(10000,[968,1719,...|[0.26806131661377...|[0.56661689925465...|       0.0|\n",
      "|RT @Pratikfc7: Ne...|(10000,[76,83,93,...|[0.12043983807487...|[0.53007361493140...|       0.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
